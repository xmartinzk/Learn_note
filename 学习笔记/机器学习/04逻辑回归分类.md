# 分类

## 回归方式解决分类问题

* 存在奇异值会严重影响回归函数。


## 逻辑回归（假的回归方法）模型

* 训练集

$$
\{x^{(1)},y^{(1)}\},\{x^{(2)},y^{(2)}\},\dots,\{x^{(m)},y^{(m)}\}
$$

* 数据格式

$$
x\in\begin{bmatrix}
    x_0\\
    x_1\\
    \vdots\\
    x_n
\end{bmatrix}
x_0=1,y\in\{0,1\}
$$
> m个训练集，n+1个训练参数
* 假设函数

$$
h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$

* 代价函数

$$
cost(h_\theta(x),y)=\{\begin{aligned}
    -\log (h_\theta(x)) && y =1\\
    -\log (1-h_\theta(x))&& y=0
\end{aligned}
$$
* 压缩版代价函数
$$
cost(h_\theta(x),y)=-y\log (h_\theta(x))-(1-y)\log (1-h_\theta(x))
$$

* 梯度下降


$$
\theta_j = \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\overrightarrow{\theta})
$$

## 特征缩放与数据归一化
> 与多元线性回归的方法一样。

## 一对多的线性回归（多元分类）

* 视为多个相互独立的二元分类，预测每种情况下，各个分类的概率，选取最高的概率。

$$
h^{(i)}_\theta(x)=P(y=i|x;\theta)\\
max\{h^{(i)}_\theta(x)\}
$$

## 编程任务1

* 构造多元分类数据集
* 对数据进行归一化处理
* 建立梯度下降公式，进行迭代。
* 选择不同学习率，观察梯度下降过程，并绘制。

## 编程任务2

* 使用python内置的高级优化算法（梯度下降之外的算法进行拟合，并对比不同算法的拟合效果）

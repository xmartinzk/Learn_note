# 多元线性回归

## 问题

* 假设函数

$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x+\theta_3x+\theta_4x\\
= [\theta_0,\theta_1,\theta_2,\theta_3,\theta_4]\times[1,x_1,x_2,x_3,x_4]^T\\
=\overrightarrow{\theta}^T\times\overrightarrow{x}
$$

* 代价函数

$$
J(\overrightarrow{\theta})=\frac{1}{2m}\sum_{i=1}^m(h_\theta(\overrightarrow{x}^{(i)})-y^{(i)})^2
$$

* 梯度下降

$$
\theta_j = \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\overrightarrow{\theta})
$$

## 特征放缩（归一化处理）

当一个假设函数的多个特征处在相同的范围的时候，函数会更快的收敛。
* 均值归一化
* 标准归一化

## 学习率

归一化之后的学习率在0-1之间。可以通过十倍差，寻找尝试寻找合适的学习率进行学习。

## 特征和多项式回归

梯度下降和以构建多项式项，用多项式的乘积项或者高阶项，进行梯度下降，一样可以拟合。
$$
y=\theta_0+\theta_1x_1^2+\theta_2x_1x_2+\theta_3x_1^2
$$

## 正规方程法

通过准确求值的方法，得到损失函数的最小值。

* X表示特征矩阵
* Y表示结果向量
* $\theta$参数向量


## 编程任务

* 使用梯度下降，拟合多个特征的多项式函数。
* 对原始数据进行归一化操作
* 选择不同的学习率观察损失函数的变换过程（收敛速度）
* 绘制梯度下降过程中，损失函数与学习率的关系
* 使用正规方程法求解多项式的系数。两者进行比较。


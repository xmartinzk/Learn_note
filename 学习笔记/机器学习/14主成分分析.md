## 降维-数据压缩

* 将两个具有强相关的维度，压缩到一个维度
  * 方便计算
  * 节约空间
  * 学习算法运行更快


## 降维-数据可视化
* 选取具有代表性的两个维度来表示数据。
* 因为数据之间存在内在关联性。GDP的例子中，国家GDP低，人均GDP低的地方，医疗健康水平等其他指数也会很低。

## 主成分分析问题规划-目标

* 本质：找到一个低维度的面，然后将数据投影到上面，使得投影误差最小（在其他方向损失的数据信息最少。）
* 均值归一化
* 特征规范化

> 对数据进行缩放，避免因为数据本身的尺度不同造成对结果的影响不同。

## 主成分分析问题规划2

* 从线性空间的角度进行理解。当有n个特征的时候，可以用n个线性无关向量作为基，表示线性空间。如果这n个特征之间存在关系（线性相关性）那么可以用其他的特征来表示这个特征，那么就可以用n-1个向量作为基表示现行空间。
* 从向量空间的角度进行理解，主成分分析，即将n为空间内的点，投影到k维子空间当中，实现降维。
* 主成分分析即将n个特征之间的线性相关性进行判定。转换为互相独立的基，消除特征之间的线性相关性。如果n个特征之间存在线性相关，那么主成分分析的特征向量是n-1个，特征值也是n-1个，其他的特征值为0。
* 在编程语言中的实现过程如下
  * 对样本进行奇异值分析。得到k个特征向量构成的特征矩阵。
  * 使用特征矩阵对每一个样本进行线性变换。（投影、降维、主成分）
  * 使用降维后的数据代入样本进行计算。

$$
SIGMA = \frac{1}{M}\sum_1^m x^{(i)}x^{(i)T} ; n\times n \\
[U,S,V]=svd(SIGMA)\\
k_{feature} = U(:,1:k)\\
k_{main} = k_{feature}^T*x  
$$

> 协方差表示两个随机变量进行线性运算时的相关性大小。

## 选取主成分的个数

* 通过特征值累计，占比例

$$
main = \frac{\sum_{i=1}^k s_{ii}}{\sum_{i=1}^n s_{ii}}
$$


## 压缩重现

* 从低维度的向量通过特征向量矩阵，还原回原来的向量。进行反向的现行变换。 

## PCA算法的应用
* 首先通过主成分分析，对数据降维。
* 然后通过特征向量矩阵对数据进行映射，形成新的数据。
* 最后运行机器学习算法，提高算法的效率。

### 主要应用
* 数据压缩
* 数据可视化，可以将数据压缩到两维或者3维，对数据进行可视化，然后分析数据的特征。
* 通过PCA降低特征数，防止过拟合的方式很愚蠢。PCA降低特征数的原理本质上是减少特征之前的现行相关性，添加新的多项式特征的原理，本质上是增加特征之间的现行相关性。这样相互冲突的操作是多余的。


## 编程任务

* 使用python完成主成分分析过程，观察分析的结果。对主城分析的结果进行分析。结合概率论和数理统计的内容，判断前k个特征向量，占主成分的多少。


### 损失函数

**损失函数**是关于模型**计算结果f(x)**和样本**实际结果y**的非负实值函数，记作L(y，f(x))，用它来解释模型在每个样本实例上的误差。

损失函数的值**越小**，说明预测值与实际值越**接近**，即模型的拟合效果**越好**。

#### 0-1损失函数：

![0-1损失函数](.\picture\0-1损失函数.png)

没什么用，不做赘述

#### L1损失函数

![L1损失函数](.\picture\L1损失函数.png)

绝对损失函数是将实际目标值y与预测值f(x)之间的差求**绝对值**，损失函数的结果为**非负**。L1损失对估计值和真实值之差取绝对值，对偏离真实值的输出**不敏感**

因此在观测中**存在异常值时**有利于保持模型**稳定**。

#### L2损失函数

![L2损失函数](.\picture\L2损失函数.png)

平方损失函数计算的是实际目标值y与预测值f(x)之间的**差的平方**，L2损失函数通过平方计算**放大**了估计值和真实值的**距离**，因此对偏离观测值的输出给予**很大的惩罚**。

平方损失函数将预测的错误放大并且给予很大的惩罚，有利于后续的工作。

此外，L2损失函数是**平滑**函数，在求解其优化问题时有利于**误差梯度**的计算。



| L2损失函数               | L1损失函数 |
| :----------------------- | :--------- |
| 不是非常的鲁棒（robust） | 鲁棒       |
| 稳定解                   | 不稳定解   |
| 总是一个解               | 可能多个解 |

其实所谓的**L1_Loss**与**L2_Loss**与**MSE（均方误差）**、**MAE（平均绝对误差）**损失函数一个**1/n**的区别，所以他们的优点和缺点是互通的



#### 对数损失函数

![对数损失函数](.\picture\对数损失函数.png)

对数损失函数用到了**极大似然估计**的思想。

**Logistic回归**的损失函数就是**对数损失**函数，在Logistic回归的推导中，它假设样本服从**伯努利**分布，然后求得满足该分布的**似然函数**，接着用对数求极值。

Logistic回归并没有求对数似然函数的最大值，而是把**极大化**当做一个**思想**，进而推导它的风险函数为**最小化**的负的似然函数。从损失函数的角度上，它就成为了log损失函数。

在极大似然估计中，通常都是先取对数再求导，再找极值点，这样做是方便计算极大似然估计。

**p(y|x)**是指在当前模型的基础上，对于输入变量x，其预测值为y，也就是**预测正确的概率**。

> ”在公式中加**负号**，表示预测正确的**概率越高**，其**损失值**应该**越小”**。





### 正则化

L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制。对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）。下图是Python中Lasso回归的损失函数，式中加号后面一项a||w||~1~即为L1正则化项——欧式距离。

![L1正则化](.\picture\L1正则化.png)

下图是Python中Ridge回归的损失函数，式中加号后面一项a||w||~2~^2^即为L2正则化项-——曼哈顿距离。

![L2正则化](.\picture\L2正则化.png)

一般回归分析中w为权重系数，L1为权重向量绝对值之和，通常表示为||w||~1~，L2平方和再求平方根，通常表示为||w||~2~  一般正则化时会添加系数

- L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择
- L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合

> 凡是减少泛化误差，而不是减少训练误差的方法，都可以称为正则化方法。
>
> 通俗来说，即**凡是能减少过拟合的方法，都是正则化方法**。




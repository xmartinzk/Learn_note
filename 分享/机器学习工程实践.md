# 机器学习工程实践

# 机器学习介绍

## 机器学习的特点

![image-20220905073545419](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/image-20220905073545419.png)

**（1）传统的编程模式**

==通过规则和数据获取答案==

**（2）机器学习模式**

==通过数据和答案得到规则==

## 机器学习的对象

​		**机器学习的对象是数据，从数据出发，提取特征，抽象出数据模型，发现数据中的规律，再回到对新数据的分析和预测。**

![image-20220905074611380](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/image-20220905074611380.png)



# 机器学习分类

## 按任务类型分类

**（1）回归问题**

回归问题就是利用数理统计中的回归分析技术，来确定两种或两种以上变量之间的依赖关系。

<img src="https://pic-1313413291.cos.ap-nanjing.myqcloud.com/image-20220905074916802.png" alt="image-20220905074916802" style="zoom:50%;" />

**（2）分类问题**

分类问题是机器学习中最常见的一类任务，利用图像分类，文本分类等。

<img src="https://pic-1313413291.cos.ap-nanjing.myqcloud.com/image-20220905075604422.png" alt="image-20220905075604422" style="zoom:67%;" />

**（3）聚类问题**

聚类问题有称群分析，目标是将样本划分为紧密关系的子集或簇，简单来说就是希望利用模型将样本数据集聚合成几大类，算是分类问题中的一种特殊情况，。

<img src="https://pic-1313413291.cos.ap-nanjing.myqcloud.com/image-20220905080609757.png" alt="image-20220905080609757" style="zoom:67%;" />

**（4）降维问题**

​		降维是指采用某种映射方法，将高维空间中的数据点映射到底维空间。为什么要使用降维？可能是原始高维空间中包含冗余信息或噪声，需要通过降维将其消除；也可能是某些数据集的特征维度过大，训练过程比较困难，需要通过降维来减少特征的量。



## 按学习方式分类

**（1）有监督学习**

​		简称监督学习，是指基于一组带有结果标注的样本训练模型，然后用该模型对新的未知结果的样本作出预测。

常见监督学习方式：分类、回归

**（2）无监督学习**

​		无监督学习中，训练样本的结果信息没有被标注，即训练集的结果标签是未知的。我们的目标是，通过对这些无标记训练样本的学习来揭示数据的内在规律，发现隐藏在数据之下的内在模式。

常见无监督学习的方式：聚类、降维

**（3）强化学习**

​		又称评价学习，是从动物学习参数扰动自适应控制等理论发展而来的，它把学习过程看做一个试探评价过程。

![image-20220905082022877](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/image-20220905082022877.png)

​		机器先选择一个初始化动作作用于环境，环境接收到该动作后状态发生变化，同时产生一个强化信号反馈给机器，机器再根据强化信号和环境当前状态选择下一个动作，选择的原则是是使受到正强化的概率增大。通俗的讲就是，让机器自己不断如尝试和探索，采取一定的趋利避害的策略，通过不断的试错和调整，最终机器会发现，哪种行为会产生最大回报，，从而学习出自己的一套较为理想的处理问题的模式，当以后再面临一些问题时，他就可以很自然的采用一种最佳模式去应对和处理。

​		强化学习是一种重要的机器学习方法，在智能控制机器人和分析预测等领域有许多应用，比如在围棋界打败世界冠军的AlphaGo就运用了强化学习。



# 模型评估指标&emsp;

## 分类模型评估指标

​		分类模型的评估指标很多，不同评估指标的侧重点可能不同，有时不同的评估指标彼此之间甚至有可能相互冲突，精度和召回率就是一对矛盾量。

### 准确率

&emsp;&emsp;准确率是用来衡量模型对数据集中的样本预测正确的比例。

&emsp;&emsp;准确率=预测正确样本数/参与预测的样本总数

```python
from sklearn.metrics import accuracy_score #引入sklearn计算库

y_true=[1,2,3,4,5,6,7,8,9,10]
y_pred=[1,2,3,4,5,6,2,2,2,2]

Accuracy=accuracy_score(y_true,y_pred,normalize=True)
```

![image-20220906135929107](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/image-20220906135929107.png)

### 混淆矩阵

> TP：真正例，被判定为正样本，事实上也是正样本；
>
> TN：真反例，被判定为负样本，事实上也是负样本；
>
> FP：假正例，被判定为正样本，但事实上是负样本；
>
> FN：假反例，被判定为负样本，但事实上是正样本
>
> **TPR（真正例率）**：被分类器正确分类为正例的正例样本数占所有真正正例样本数的比例，即TPR = TP / (TP + FN)
>
> **FPR（假正例率）**：被分类器错误分类为正例的负例样本数占所有真正负例样本数的比例，即FPR = FP / (FP + TN)

![image-20220906141838030](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/image-20220906141838030.png)

&emsp;&emsp;精度指的是所有预测为正例的样本（TP+FP）中真正为正例的样本（TP）的比率，一般来说，就是你找到的信息中真正是我想要的有多少，又叫查准率。



### 查全率（recall）

&emsp;&emsp;表示在所有实际正向数据中，有多少预测正确了。

​		Rec=TP/(TP+FN)

​		也称召回率

### 查准率（precision）

​		被分类器正确分类为正例的正例样本数占所有被分类器判定为正例的样本数的比例。查准率可以用来评估分类器的精度，即分类器在分类为正例的样本中有多少是真正的正例。	

### F1值（F1-score）

​		查准率和查全率是一对矛盾的指标，**一般说，当查准率高的时候，查全率一般很低；查全率高时，查准率一般很低。**

​		比如：如反洗钱中，查全率越高代表判定为可疑客户的人数越多，当然其中真正为黑例的数量肯定会增大，与此同时，被“冤枉的人”也会怎多，这样准确率必然下降，即查准率下降。

​		在实际的模型评估中，单用Precision或者Recall来评价模型是不完整的，评价模型时必须用Precision/Recall两个值。这里介绍三种使用方法：F1度量

​		F1度量是Precision和Recall的调和平均数，表示模型预测正确的正例占所有预测为正例的样本数的比例。F1度量综合考虑了Precision和Recall两个指标，能够较好地评估模型的性能，尤其适用于在不同Precision和Recall之间进行权衡的情况下。




### ROC曲线

​		**ROC曲线就是将真正率(TPR)作为纵轴，假正率(FPR)作为横轴所绘制的图形。**

​		我们所期望的是，TP越大越好，FP越小越好。



![image-20220906151404719](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/image-20220906151404719.png)

==如果一条曲线包裹另一条曲线，则外面曲线的性能更优==

### AUC

​		当两条ROC曲线发生交叉时，谁的性能更优就难以判定了，这时就要根据两条ROC曲线下面的面积大小来比较判断，即面积大者相对更优，**这个ROC曲线下面的面积就是AUC**。



# 数据挖掘项目流程

**（1）业务理解**

**（2）数据分析**

**（3）特征工程**

**（4）模型选择**

**（5）模型评估**

**（6）项目落地**

## 数据预处理

### 数据清洗

```python
import pandas as pd
import numpy as np

# 构造测试数据
df = pd.DataFrame({'A': [1, 2, np.nan, 4, np.nan],
                   'B': [5, np.nan, 7, np.nan, 9],
                   'C': [np.nan, np.nan, 10, 11, np.nan]})

#直接删除缺失数据
new_df_1=df.dropna()

#固定值填充
df['A'].fillna(12345,inplace=True)

# 缺失值均值填充
fill_value = df['A'].mean()
df['B'].fillna(fill_value, inplace=True)

# 缺失值中位数填充
fill_value = df['B'].median()
df['C'].fillna(fill_value, inplace=True)

# 数据替换
df.replace(np.nan,-99999)
```

## 特征工程

### 特征处理

**1.归一化和标准化**

- MinMaxScaler

  是对原始数据的线性变换，使得结果值映射到[0,1]之间。

  这种归一化比较适用在数值较集中的情况。但是这种方法有一个缺陷，就是如果max和min不稳定的时候，很容易使得归一化的结果不稳定，易受极值影响，影响后续使用效果。所以在实际应用中，我们一般用经验常量来替代max和min。

  ```python
  from sklearn.preprocessing import MinMaxScaler
  scaler = MinMaxScaler()
  scaled_data = scaler.fit_transform(data)
  ```

- StandardScaler

  将每个特征缩放到均值为0，方差为1的区间内，可以消除数据集的噪声或异常值，有利于提高模型的鲁棒性

  ```python
  from sklearn.preprocessing import StandardScaler
  scaler = StandardScaler()
  scaled_data = scaler.fit_transform(data)
  ```

**2.离散化**

​		就是把无限空间中有限的个体映射到有限的空间中。数据离散化操作大多是针对连续数据进行的，**处理之后的数据值域分布将从连续属性变为离散属性。**如年龄特征，0-30设为1，30-60设为2，60以上设为3，能有效避免异常值对于模型的干扰，提高鲁棒性。

- 离散化方式会影响后续数据建模和应用效果：

- 使用决策树往往倾向于少量的离散化区间，过多的离散化将使得规则过多受到碎片区间的影响。

- 关联规则需要对所有特征一起离散化，关联规则关注的是所有特征的关联关系，如果对每个列单独离散化将失去整体规则性。

### 特征交互

​		特征交互就是人为的或者通过构造模型，自动将两个或两个两个以上的特征进行交互常用的交互方式有，求和、相乘、取对数等。

### 特征映射

​		特征映射是一个比特征交互更高级的问题，一般通过某些机器学习模型来实现。

## 模型选择与模型调优

### 模型选择

**（1）数据集切分**

​		使用cross_validation类中的train_test_split函数可以很容易地将原始数据按照比例切成训练集和验证集。

**（2）交叉验证**

​		也是一种模型验证技术。简单来说就是重复使用数据。除去测试集，把剩余数据进行划分，组合成多组不同的训练集和验证集，某次在训练集中出现的样本下次可能成为验证集中的样本，这就是所谓的“交叉”。最后用各次验证误差的平均值作为模型最终的验证误差。

![image-20220906162328649](https://pic-1313413291.cos.ap-nanjing.myqcloud.com/image-20220906162328649.png)

​		如果验证集较大，那么训练集就会变得很小，如果数据集本身就不大的话，显然这样训练出来的模型就不好。如果验证集很小，那么此验证误差就不能很好地反映出泛化误差。此外，在不同的划分方式下，训练出的不同模型的验证误差波动也很大（方差大）。到底以哪次验证误差为准？谁都不知道。但是如果将这些不同划分方式下训练出来的模型的验证过程重复多次，得到的平均误差可能就是对泛化误差的一个很好的近似。

### 模型调优

**在模型选定后，一般还需要进行模型的参数调优，这里我们介绍，网格搜索寻优和随机搜索寻优**

**（1）网格搜索寻优**

​		`GridSearchCV`

​		网格搜索寻优是一种暴力寻优方法，做法是将某些参数放在网格中，通过遍历的方式，用交叉验证对参数空间进行求解，寻找最佳的参数。

**（2）随机搜索寻优**

​		`RandomizedSearchCV`

​		在参数较少的时候，采用暴力寻优是可以的，但是当参数过多的时候，或者当参数为连续值时候，暴力寻优明显不大可取，所以提出随机搜索寻优的方式，其做法是对这些连续值进行采样，从中挑选出一些值作为代表。



